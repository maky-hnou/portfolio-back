"""Module providing the ChatHandler class for managing chat interactions.

This module includes the `ChatHandler` class, which handles interactions with a language model and a
vector database. It manages conversation formatting, querying embeddings, and processing responses from
the AI model. The class uses various configurations and utility functions to ensure smooth conversation flow.

Classes:
    ChatResult: A named tuple representing the result of a chat interaction, including the system message, AI message, and off-topic response count.
    ChatHandler: A class for managing chat interactions and responses from a language model.

Dependencies:
    - typing.NamedTuple: Type hint for defining named tuples.
    - langchain_core.messages: Classes for representing different types of messages (AI, Human, System).
    - langchain_openai: Classes for interacting with OpenAI models and embeddings.
    - loguru.logger: Logger for logging messages and events.
    - portfolio_backend.services.chat.config: Configuration parameters related to chat handling.
    - portfolio_backend.services.embeddor.embeddings: Embedding utility for generating vector representations of text.
    - portfolio_backend.settings: Settings for accessing configuration parameters.
    - portfolio_backend.vdb.configs: Configuration parameters for the vector database.
    - portfolio_backend.vdb.milvus_connector: Class for connecting and interacting with the Milvus vector database.
    - portfolio_backend.web.api.message.schema: Schemas for defining message data structures.
"""

from typing import NamedTuple

from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from loguru import logger

from portfolio_backend.services.chat.config import (
    limit_length_message,
    limit_out_of_topic_message,
    messages_limit,
    off_topic_count_limit,
    off_topic_response,
    prompt,
)
from portfolio_backend.services.embeddor.embeddings import Embedding
from portfolio_backend.settings import settings
from portfolio_backend.vdb.configs import vdb_config
from portfolio_backend.vdb.milvus_connector import MilvusDB
from portfolio_backend.web.api.message.schema import MessageBy, MessageDTO


class ChatResult(NamedTuple):
    """Result of a chat interaction.

    Attributes:
        system_message (str | None): The system-generated message, if any.
        ai_message (str): The message generated by the AI.
        off_topic_response_count (int): The count of off-topic responses encountered.

    """

    system_message: str | None
    ai_message: str
    off_topic_response_count: int


class ChatHandler:
    """Handler for managing chat interactions with AI and a vector database.

    Attributes:
        llm_model (ChatOpenAI): The language model for generating responses.
        milvus_db (MilvusDB): The vector database instance for querying embeddings.
        embedding_model (OpenAIEmbeddings): Model for generating embeddings from messages.
        message_type_map (dict): Mapping of message types to their respective classes.

    """

    def __init__(self, llm_model: ChatOpenAI, milvus_db: MilvusDB):
        """Initialize the ChatHandler with a language model and a vector database.

        Args:
            llm_model (ChatOpenAI): The language model used for generating AI responses.
            milvus_db (MilvusDB): The vector database instance for embedding queries.
        """
        logger.info("Initializing ChatHandler with LLM and MilvusDB")
        self.llm_model = llm_model
        self.milvus_db = milvus_db
        self.embedding_model = OpenAIEmbeddings(
            model=settings.embedding_model,
            openai_api_key=settings.openai_api_key,  # type: ignore
        )
        self.message_type_map = {
            MessageBy.HUMAN: HumanMessage,
            MessageBy.AI: AIMessage,
            MessageBy.SYSTEM: SystemMessage,
        }

    def _format_message(self, message: MessageDTO) -> BaseMessage:
        """Format a MessageDTO into a BaseMessage.

        Args:
            message (MessageDTO): The message to format.

        Returns:
            BaseMessage: The formatted message.

        Raises:
            ValueError: If the message type is unexpected.
        """
        try:
            return self.message_type_map[message.message_by](content=message.message_text)
        except KeyError as e:
            raise ValueError(f"Unexpected message_by value: {message.message_by}") from e

    def _update_conversation(
        self,
        old_conversation: list[MessageDTO],
        human_query: MessageDTO,
        system_message: str,
    ) -> list[BaseMessage]:
        """Update the conversation with a new human query and system message.

        Args:
            old_conversation (list[MessageDTO]): The existing conversation history.
            human_query (MessageDTO): The new human message to add.
            system_message (str): The system message to append to the conversation.

        Returns:
            list[BaseMessage]: The updated conversation including the new messages.
        """
        logger.info(f"Updating conversation with new human query: {human_query.message_text}")
        old_conversation.append(human_query)
        # recreate the conversation by creating a list of responses (by system, ai, human)
        formatted_conversation = [self._format_message(msg) for msg in old_conversation]
        formatted_conversation.append(SystemMessage(system_message))
        logger.debug(f"Conversation updated with system message: {system_message}")
        return formatted_conversation

    async def handle_chat(
        self,
        conversation: list[MessageDTO],
        human_message: MessageDTO,
        off_topic_response_count: int,
    ) -> ChatResult:
        """Handle chat interaction and generate AI response.

        Args:
            conversation (list[MessageDTO]): The existing conversation history.
            human_message (MessageDTO): The latest message from the human user.
            off_topic_response_count (int): The current count of off-topic responses.

        Returns:
            ChatResult: The result of the chat interaction, including system message, AI message, and updated off-topic count.
        """
        logger.info(
            f"Handling chat for message: {human_message.message_text} with off-topic count: {off_topic_response_count}",
        )

        # if len messages > 30, return limit length message
        if len(conversation) > messages_limit:
            logger.warning(f"Conversation length exceeded limit of {messages_limit}.")
            ai_message = limit_length_message
            system_message = None
        # if off-topic count < 3, embed the last message and use it to query the vector database
        elif off_topic_response_count < off_topic_count_limit:
            logger.info(f"Embedding message for vector search: {human_message.message_text}")
            query_embedding = Embedding(text=human_message.message_text, embedding_model=self.embedding_model)
            query_result = self.milvus_db.search(
                collection_name=vdb_config.collection_name,
                search_data=[query_embedding.text_embedding],
                limit=vdb_config.topk,
                output_fields=["text"],
                search_params=vdb_config.search_params,
                threshold=vdb_config.threshold,
            )
            logger.info(f"Query result from MilvusDB: {query_result}")
            system_message = prompt.format(context=query_result)
            formatted_conversation = self._update_conversation(
                old_conversation=conversation,
                human_query=human_message,
                system_message=system_message,
            )
            # send the conversation to openai api and get the response
            logger.info("Sending conversation to LLM for response.")
            response = self.llm_model.invoke(input=formatted_conversation)
            ai_message = response.content  # type: ignore
            logger.info(f"Received AI message: {ai_message}")

            # if the response is None, increment off-topic count and send off-topic message as response
            if "null" in ai_message.lower():
                logger.warning("AI response was off-topic.")
                off_topic_response_count += 1
                ai_message = off_topic_response.format(off_topic_count=off_topic_response_count)
        # else (if off-topic count exceeds 3), send the off-topic message
        else:
            logger.warning("Off-topic count exceeded the limit.")
            ai_message = limit_out_of_topic_message
            system_message = None

        # return the message + off-topic count
        logger.info(
            f"Returning system message: {system_message}, AI message: {ai_message}, off-topic count: {off_topic_response_count}",
        )
        return ChatResult(system_message, ai_message, off_topic_response_count)
